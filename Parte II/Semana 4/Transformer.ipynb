{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgmCmsSWHVjM"
   },
   "source": [
    "# **Implementación de un Transformer Decoder tipo GPT para Generación de Texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmWavD1qE9Ao"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx_QDKgr-UtK"
   },
   "source": [
    "## **Tokenización letra por letra (caracter a entero)**\n",
    "\n",
    "La tokenización del texto se realiza a nivel de **caracteres**, no de palabras ni n-conjuntos de palabras. El objetivo es asignar a cada carácter único un identificador numérico entero, usando un mapeo basado en el orden alfabético.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "1. **Obtener los caracteres únicos del corpus**\n",
    "   - Se extraen todos los caracteres únicos presentes en el texto.\n",
    "   - Por ejemplo, si el texto es `\"Hola.\"`, los caracteres únicos serían `['H', 'o', 'l', 'a','.']`.\n",
    "\n",
    "2. **Ordenar alfabéticamente**\n",
    "   - Los caracteres únicos se ordenan de forma alfabética para asignar los índices de forma determinista.\n",
    "   - Ejemplo: `['.','H', 'a', 'l', 'o']`\n",
    "\n",
    "3. **Mapeo carácter a índice**\n",
    "   - Se crea un diccionario `char2idx` que asigna a cada carácter un número entero.\n",
    "   - Por ejemplo:\n",
    "     ```python\n",
    "     char2idx = {'.': 0, 'H': 1, 'a': 2, 'l': 3, 'o': 4}\n",
    "     ```\n",
    "\n",
    "4. **Mapeo inverso índice a carácter**\n",
    "   - También se crea `idx2char` para decodificar las predicciones del modelo:\n",
    "     ```python\n",
    "     idx2char = {0: '.', 1: 'H', 2: 'a', 3: 'l', 4: 'o'}\n",
    "     ```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pK2Stf4vE-Tb"
   },
   "outputs": [],
   "source": [
    "# Texto corto para prueba\n",
    "text = (\n",
    "    \"La National Football League (NFL) es la principal liga profesional de fútbol americano en los Estados Unidos. \"\n",
    "    \"Fue fundada en 1920 como la American Professional Football Association y cambió su nombre a NFL en 1922. \"\n",
    "    \"Está compuesta por 32 equipos divididos en dos conferencias: la American Football Conference (AFC) y la National Football Conference (NFC). \"\n",
    "    \"Cada conferencia tiene cuatro divisiones: Norte, Sur, Este y Oeste. \"\n",
    "    \"Los equipos compiten durante una temporada regular de 17 semanas para clasificar a los playoffs, que culminan en el Super Bowl, \"\n",
    "    \"uno de los eventos deportivos más vistos a nivel mundial. \"\n",
    "\n",
    "    \"Equipos legendarios como los Pittsburgh Steelers, con seis títulos de Super Bowl, y los New England Patriots, también con seis campeonatos, \"\n",
    "    \"han dejado una huella imborrable en la historia del deporte. \"\n",
    "    \"Franquicias como los Dallas Cowboys son conocidos como 'America’s Team', mientras que los Green Bay Packers, con su base de fans comunitaria, \"\n",
    "    \"son famosos por su legado y tradición. \"\n",
    "\n",
    "    \"El fútbol americano se juega entre dos equipos de 11 jugadores cada uno. \"\n",
    "    \"El objetivo es avanzar el balón en la zona de anotación del equipo rival, lo cual se puede lograr por tierra (acarreos) o por aire (pases). \"\n",
    "    \"Cada equipo tiene cuatro oportunidades (downs) para avanzar 10 yardas. \"\n",
    "    \"Si lo logran, obtienen una nueva serie de downs. Si no, deben despejar (punt) o intentar un gol de campo si están en posición. \"\n",
    "\n",
    "    \"El mariscal de campo o quarterback (QB) es la pieza clave de la ofensiva. Figuras como Tom Brady, Peyton Manning y Patrick Mahomes \"\n",
    "    \"han redefinido el papel del QB en distintas épocas. \"\n",
    "    \"En la defensa, jugadores como Ray Lewis, J.J. Watt y Aaron Donald han dominado con su capacidad para leer jugadas y detener al rival. \"\n",
    "\n",
    "    \"Las posiciones se dividen en ofensiva (QB, RB, WR, TE, OL), defensiva (DL, LB, CB, S) y equipos especiales (K, P, LS, KR, PR). \"\n",
    "    \"El reglamento incluye penalizaciones como interferencia de pase, sujeción y formación ilegal. \"\n",
    "    \"El reloj de juego, el manejo de tiempos fuera y las estrategias de reloj son fundamentales en los últimos minutos del partido. \"\n",
    "\n",
    "    \"La NFL celebra eventos clave como el Draft, el Combine, el Pro Bowl y el Super Bowl. \"\n",
    "    \"Además, cuenta con iniciativas de responsabilidad social, como 'Crucial Catch' para la lucha contra el cáncer, y 'My Cause My Cleats', \"\n",
    "    \"que permite a los jugadores expresar causas personales. \"\n",
    "\n",
    "    \"La rivalidad entre equipos es una parte esencial de la cultura de la NFL. Enfrentamientos como Packers vs. Bears, Cowboys vs. Eagles, \"\n",
    "    \"y Patriots vs. Jets tienen décadas de historia e intensidad. \"\n",
    "\n",
    "    \"El Super Bowl LVIII se llevó a cabo en el Allegiant Stadium de Las Vegas, Nevada. Patrick Mahomes lideró a los Kansas City Chiefs a una victoria ajustada, \"\n",
    "    \"confirmando su estatus como uno de los mejores quarterbacks de la nueva generación. \"\n",
    "\n",
    "    \"Con transmisiones en más de 180 países y una base de fans global en expansión, la NFL continúa siendo un símbolo de estrategia, potencia, \"\n",
    "    \"resistencia y emoción cada domingo. \"\n",
    "\n",
    "    \"Desde los estadios llenos de historia como Lambeau Field y Arrowhead Stadium, hasta las modernas instalaciones como SoFi Stadium, \"\n",
    "    \"el espectáculo del fútbol americano combina tradición con innovación. \"\n",
    "    \"Miles de analistas, comentaristas y fanáticos discuten semana a semana cada jugada, cada decisión de los entrenadores y cada trade en la liga. \"\n",
    "    \"La fantasy football ha creado una nueva forma de interacción entre el fan y el deporte, llevando el análisis estadístico a un nivel más profundo. \"\n",
    "    \"Y a pesar de las lesiones, las derrotas y los momentos difíciles, la esencia del juego permanece: once contra once, luchando y soñando por avanzar una yarda más.\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Crear vocabulario de caracteres\n",
    "chars = sorted(list(set(text))) # Todos los caractéres presentes en el texto ordenados\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# Mapas para codificación\n",
    "char2idx = {u: i for i, u in enumerate(chars)} # Asigna un número entre 0 y 43 (44 caracteres) a cada string en chars\n",
    "idx2char = np.array(chars) #Da una arreglo de numpy con el string de cada caracter en las entradas\n",
    "print(char2idx)\n",
    "print(idx2char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa7xYc2ijJbe"
   },
   "outputs": [],
   "source": [
    "# Funciones de codificación (usando el mapeo de codificación)\n",
    "def encode(s): return np.array([char2idx[c] for c in s]) # Según el mapa de codificación char2idx codifica un string\n",
    "print(encode('Cómo funciona encode'))\n",
    "def decode(arr): return ''.join([idx2char[i] for i in arr])\n",
    "print(decode([14, 55, 65, 0, 42, 57, 50, 39, 45, 51, 50, 37, 0, 40, 41, 39, 51, 40, 41]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCceEBd8_gZ0"
   },
   "source": [
    "### Preparación de datos para el entrenamiento\n",
    "\n",
    "Para entrenar al modelo, se divide el texto en **bloques de longitud fija**, conocida como **longitud de contexto** o `block_size`. Cada bloque consiste en una secuencia de tokens consecutivos del corpus.\n",
    "\n",
    "El objetivo del modelo es **predecir el siguiente token** dado un contexto anterior. Por ello, el conjunto de entrenamiento se construye de la siguiente forma:\n",
    "\n",
    "- Cada **input** es una secuencia de `block_size` tokens.\n",
    "- Cada **target** (salida deseada) es la misma secuencia desplazada una posición hacia la izquierda, es decir, el token que sigue a cada uno de los tokens de entrada.\n",
    "\n",
    "Por ejemplo, si se trabaja con bloques de longitud 50, el corpus se segmenta en pares input/target como sigue:\n",
    "\n",
    "- **Input**: caracteres del índice `i` al `i+49` (50 caracteres).\n",
    "- **Target**: caracteres del índice `i+1` al `i+50` (también 50 caracteres).\n",
    "\n",
    "Esto permite que el modelo aprenda a predecir cada carácter basándose en el contexto anterior inmediato.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWSyxUgtjMGF"
   },
   "outputs": [],
   "source": [
    "# Secuencias\n",
    "block_size = 50  # longitud del contexto\n",
    "step = 1\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "# Genera listas con 50 caracteres, cada lista es igual a la anterior con un caracter menos al principio y uno posterior al\n",
    "for i in range(0, len(text) - block_size):\n",
    "    inputs.append(encode(text[i:i+block_size]))\n",
    "    targets.append(encode(text[i+1:i+block_size+1]))\n",
    "\n",
    "print('Input 1:',decode(inputs[0]))\n",
    "print(\"Target 1:\",decode(targets[0]))\n",
    "print('El input siguiente es igual al target previo')\n",
    "print('Input 2:',decode(inputs[1]))\n",
    "print('Target 2:',decode(targets[1]))\n",
    "\n",
    "x = np.array(inputs)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVbwH_TnNLib"
   },
   "source": [
    "## **Positional Encoding**\n",
    "\n",
    "En los modelos Transformer, no se utilizan arquitecturas recurrentes ni convolucionales, por lo que es necesario proporcionar explícitamente información de **posición** al modelo para que pueda distinguir el orden de las entradas.\n",
    "\n",
    "La clase `PositionalEncoding` implementa una codificación posicional senoidal descrita en el paper _\"Attention is All You Need\"_ (Vaswani et al., 2017). Esta codificación se suma a las entradas del decoder en este caso para incorporar información sobre el orden de las secuencias.\n",
    "\n",
    "Para cada posición $ \\text{pos} $ y cada dimensión $ i $ del embedding, se calcula:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos}, 2i} = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos}, 2i+1} = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "Estas funciones sinusoidales de diferentes frecuencias permiten que el modelo:\n",
    "\n",
    "- Distinga posiciones relativas y absolutas.\n",
    "- Generalice a secuencias más largas que las vistas durante el entrenamiento.\n",
    "\n",
    "# Ejemplo numérico: Entrada \"Hola\" al Transformer Decoder\n",
    "\n",
    "## Tokens y IDs\n",
    "| Token | ID  |\n",
    "|-------|-----|\n",
    "| 'H'   | 0   |\n",
    "| 'o'   | 1   |\n",
    "| 'l'   | 2   |\n",
    "| 'a'   | 3   |\n",
    "\n",
    "## Embeddings (dimensión = 2)\n",
    "| ID  | Embedding          |\n",
    "|-----|--------------------|\n",
    "| 0   | [1.0, 0.5]         |\n",
    "| 1   | [0.4, 1.2]         |\n",
    "| 2   | [0.7, 0.3]         |\n",
    "| 3   | [1.1, 0.9]         |\n",
    "\n",
    "## Positional Encoding (fija con seno y coseno)\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "PE_{(pos,0)} = \\sin(pos) \\\\\n",
    "PE_{(pos,1)} = \\cos\\left(\\frac{pos}{100}\\right)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "| Posición | PE(pos,0) = sin(pos) | PE(pos,1) = cos(pos/100) |\n",
    "|----------|---------------------|--------------------------|\n",
    "| 0        | 0.0000              | 1.0000                   |\n",
    "| 1        | 0.8415              | 0.9950                   |\n",
    "| 2        | 0.9093              | 0.9801                   |\n",
    "| 3        | 0.1411              | 0.9553                   |\n",
    "\n",
    "## Suma Embedding + Positional Encoding\n",
    "\n",
    "| Token | Embedding    | Positional Encoding | Suma (Input al decoder)     |\n",
    "|-------|--------------|---------------------|-----------------------------|\n",
    "| 'H'   | [1.0, 0.5]   | [0.0000, 1.0000]    | [1.0000 + 0.0000, 0.5 + 1.0] = [1.0000, 1.5000]   |\n",
    "| 'o'   | [0.4, 1.2]   | [0.8415, 0.9950]    | [0.4 + 0.8415, 1.2 + 0.9950] = [1.2415, 2.1950]   |\n",
    "| 'l'   | [0.7, 0.3]   | [0.9093, 0.9801]    | [0.7 + 0.9093, 0.3 + 0.9801] = [1.6093, 1.2801]   |\n",
    "| 'a'   | [1.1, 0.9]   | [0.1411, 0.9553]    | [1.1 + 0.1411, 0.9 + 0.9553] = [1.2411, 1.8553]   |\n",
    "\n",
    "---\n",
    "\n",
    "**Resultado final (matriz de entrada al Transformer Decoder):**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1.0000 & 1.5000 \\\\\n",
    "1.2415 & 2.1950 \\\\\n",
    "1.6093 & 1.2801 \\\\\n",
    "1.2411 & 1.8553\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMdHASGsFCWT"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, maxlen, d_model):\n",
    "        super().__init__()\n",
    "        pos = np.arange(maxlen)[:, np.newaxis]  # Posiciones (shape: [maxlen, 1])\n",
    "        i = np.arange(d_model)[np.newaxis, :]   # Dimensiones del embedding (shape: [1, d_model])\n",
    "\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        angle_rads = pos * angle_rates  # Producto de posición y frecuencia angular\n",
    "\n",
    "        # Aplicar sin a índices pares (0, 2, 4, ...)\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        # Aplicar cos a índices impares (1, 3, 5, ...)\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)  # [1, maxlen, d_model]\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: Tensor de entrada con shape [batch_size, seq_len, d_model]\n",
    "        # Retorna la suma del embedding con la codificación posicional correspondiente\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2FSwfaqOYSB"
   },
   "source": [
    "## **Causal Mask**\n",
    "\n",
    "En el Transformer Decoder, se utiliza una **máscara causal (causal mask)** para evitar que un token \"vea el futuro\". Durante el entrenamiento de modelos de lenguaje es importante, ya que se busca que la predicción del siguiente token se base solo en los tokens anteriores o actuales, no en los futuros.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuLgRhJLZtC4"
   },
   "outputs": [],
   "source": [
    "def causal_mask(block_size):\n",
    "    mask = tf.linalg.band_part(tf.ones((block_size, block_size)), -1, 0)  # 1s diagonal y abajo\n",
    "    mask = tf.expand_dims(tf.expand_dims(mask, 0), 0)  # (1,1,block_size,block_size)\n",
    "    return tf.cast(mask, tf.bool)\n",
    "\n",
    "print(causal_mask(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoPRtmyLBlkA"
   },
   "source": [
    "### Verificación del funcionamiento de la máscara causal\n",
    "\n",
    "Para comprobar cómo actúa la **máscara causal** al aplicarse en una capa de atención, se puede comparar la salida de la red con y sin dicha máscara.\n",
    "\n",
    "Se procede de la siguiente manera:\n",
    "\n",
    "1. **Se define una máscara sin restricciones**, es decir, una matriz donde todos los elementos son válidos (equivalente a permitir atención total entre todos los tokens).\n",
    "\n",
    "2. **Se define una máscara causal**, donde solo se permite que cada token atienda a sí mismo y a los tokens anteriores en la secuencia (bloque inferior triangular).\n",
    "\n",
    "3. Ambas máscaras se aplican a una misma entrada a través de una capa de atención.\n",
    "\n",
    "4. Se comparan las **salidas (outputs)** o los **pesos de atención** generados por la capa.\n",
    "\n",
    "---\n",
    "\n",
    "#### Observación esperada\n",
    "\n",
    "- Con la **máscara causal**, los pesos de atención asignados a posiciones futuras (a la derecha en la matriz) serán **cero**, lo que refleja que un token **no puede \"ver hacia el futuro\"**.\n",
    "  \n",
    "- Con la **máscara libre (sin restricciones)**, los tokens pueden atender libremente a cualquier posición, por lo tanto, **no hay ceros obligatorios** en la matriz de atención.\n",
    "\n",
    "Esto permite verificar visualmente y numéricamente que la máscara causal está funcionando correctamente dentro del mecanismo de atención.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p429U3rlBhnw"
   },
   "outputs": [],
   "source": [
    "# Máscara sin restricción (todo True)\n",
    "def full_mask(block_size):\n",
    "    mask = tf.ones((1, 1, block_size, block_size), dtype=tf.bool)\n",
    "    return mask\n",
    "\n",
    "block_size_test = 4\n",
    "embed_dim_test = 4\n",
    "num_heads_test = 2\n",
    "\n",
    "mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads_test, key_dim=embed_dim_test // num_heads_test)\n",
    "\n",
    "# Crear máscaras\n",
    "mask = causal_mask(block_size_test)\n",
    "mask_full = full_mask(block_size_test)\n",
    "\n",
    "# Input fijo (batch=1, seq_len=4, embed_dim=4)\n",
    "x_test = tf.constant([[[1., 1., 1., 1.],\n",
    "                  [2., 2., 2., 2.],\n",
    "                  [3., 3., 3., 3.],\n",
    "                  [4., 4., 4., 4.]]])\n",
    "\n",
    "\n",
    "# Atención con máscara causal y obtener scores\n",
    "out_causal, attn_scores_causal = mha(x_test, x_test, attention_mask=mask, return_attention_scores=True)\n",
    "\n",
    "# Atención con máscara full y obtener scores\n",
    "out_full, attn_scores_full = mha(x_test, x_test, attention_mask=mask_full, return_attention_scores=True)\n",
    "\n",
    "print(\"Pesos de atención con máscara causal (shape):\", attn_scores_causal.shape)\n",
    "print(attn_scores_causal.numpy())\n",
    "\n",
    "print(\"\\nPesos de atención sin máscara (shape):\", attn_scores_full.shape)\n",
    "print(attn_scores_full.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWTp1qmyj3ab"
   },
   "source": [
    "## **Definición del Modelo: `build_model`**\n",
    "\n",
    "La función `build_model(seq_len, vocab_size)` construye un modelo autoregresivo basado en un **Transformer Decoder**. Su objetivo es predecir el siguiente token en una secuencia de texto.\n",
    "\n",
    "El modelo recibe secuencias de longitud `seq_len`, donde cada token ha sido transformado en un índice entero perteneciente a un vocabulario de tamaño `vocab_size`.\n",
    "\n",
    "### Componentes principales:\n",
    "\n",
    "- **Embedding Layer**: Mapea cada índice del vocabulario a un vector de dimensión fija (`d_model`), lo que permite representar los tokens de manera continua.\n",
    "  \n",
    "- **Positional Encoding**: Como el Transformer no tiene una estructura secuencial inherente, se añade codificación posicional al embedding para introducir información del orden en la secuencia.\n",
    "\n",
    "- **Causal Mask**:\n",
    "  \n",
    "  La máscara causal evita que el modelo \"vea el futuro\". Se implementa como una matriz triangular inferior que impide que cada token atienda a tokens posteriores a él.\n",
    "  \n",
    "  Esto es crucial en tareas de predicción secuencial, donde se quiere que el modelo aprenda a predecir el siguiente token sin tener acceso a los futuros.\n",
    "\n",
    "- **MultiHeadAttention**: Permite al modelo enfocarse en diferentes partes de la secuencia en paralelo, respetando la máscara causal.Dado un conjunto de vectores de consulta (**Q**), clave (**K**) y valor (**V**), el mecanismo de atención escalada se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}} + \\text{mask}\\right) V\n",
    "$$\n",
    "\n",
    "\n",
    "- **Feed Forward Layer**: Dos capas densas que transforman las salidas del bloque de atención.\n",
    "\n",
    "- **Dropout y Normalización**: Se incluyen capas de `Dropout` para evitar overfitting y `LayerNormalization` para estabilizar el entrenamiento.\n",
    "\n",
    "- **Capa de salida**: Una capa `Dense` con `vocab_size` unidades y softmax, que predice la probabilidad de cada palabra del vocabulario como siguiente token.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdvmY6DEM2d-"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, block_size, d_model=64, num_heads=2, ff_dim=128):\n",
    "    inputs = layers.Input(shape=(block_size,))\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=d_model)(inputs)\n",
    "    x = PositionalEncoding(block_size, d_model)(x)\n",
    "\n",
    "    dropout_rate = 0.1  # El 10% de las neuronas se \"desactivan\" en las capas de dropout para evitar sobreajuste\n",
    "\n",
    "    # Multi-head self attention con máscara causal\n",
    "    mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "    mask_ = causal_mask(block_size)\n",
    "    attn_out = mha(x, x, attention_mask=mask_)\n",
    "    attn_out = layers.Dropout(dropout_rate)(attn_out)  # Dropout en salida de atención\n",
    "    x = layers.Add()([x, attn_out]) #Suma elemento a elemento dos tensores: el tensor x original y el resultado de la atención attn_out.\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Feed Forward\n",
    "    ff_out = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff_out = layers.Dropout(dropout_rate)(ff_out)  # Dropout en salida de la primera densa\n",
    "    ff_out = layers.Dense(d_model)(ff_out)\n",
    "    ff_out = layers.Dropout(dropout_rate)(ff_out)  # Dropout en salida de la segunda densa\n",
    "    x = layers.Add()([x, ff_out])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3FgU3cG9ssc"
   },
   "source": [
    "## **Construcción y entrenamiento del modelo**\n",
    "\n",
    "Este bloque de código crea, compila y entrena un modelo de red neuronal para tareas de predicción de secuencias (por ejemplo, generación de texto o modelado de lenguaje).\n",
    "\n",
    "- `optimizer='adam'`: Se utiliza el optimizador Adam, una combinación de los métodos de descenso de gradiente con momento y RMSProp.\n",
    "\n",
    "- `loss='sparse_categorical_crossentropy'`: Se emplea esta función de pérdida porque el modelo realiza una clasificación multiclase, donde el objetivo (target) es un índice entero que representa un token.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpI9s-PIFFNw"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=vocab_size, block_size=block_size)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FzPmBiFZGT2j"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, y[..., np.newaxis], epochs=200, verbose=2) # El batch-size se establece como 32 si no se indica otra cosa\n",
    "print(\"Entrenamiento finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kPAraYG8Gy-"
   },
   "source": [
    "## **Función `generate_text`**\n",
    "\n",
    "Esta función genera una secuencia de texto carácter por carácter (o token por token), utilizando el modelo entrenado. A partir de una cadena inicial (`start_string`), predice `num_chars` caracteres adicionales uno a uno.\n",
    "\n",
    "### Parámetros de entrada:\n",
    "\n",
    "- **`model`**: Modelo entrenado que recibe una secuencia de tokens de longitud fija (`block_size`) y predice los logits para el siguiente token.\n",
    "- **`start_string`**: Cadena de texto inicial a partir de la cual se comienza a generar texto.\n",
    "- **`num_chars`**: Número de caracteres (o tokens) a generar adicionalmente.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NTjFv-HBFH6G"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_chars=100):\n",
    "    input_eval = encode(start_string)\n",
    "    generated = list(input_eval)\n",
    "\n",
    "    for _ in range(num_chars):\n",
    "        # Preparar input (padded o recortado al tamaño del bloque)\n",
    "        input_seq = generated[-block_size:]\n",
    "        if len(input_seq) < block_size:\n",
    "            input_seq = [0] * (block_size - len(input_seq)) + input_seq\n",
    "\n",
    "        input_seq = np.array(input_seq)[np.newaxis, :]  # shape (1, block_size)\n",
    "        preds = model.predict(input_seq, verbose=0)\n",
    "        next_token_logits = preds[0, -1]\n",
    "\n",
    "        # Elegir el siguiente token\n",
    "        next_token = np.argmax(next_token_logits)\n",
    "\n",
    "\n",
    "        generated.append(next_token)\n",
    "\n",
    "    return decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5atlYQDe5SMc"
   },
   "outputs": [],
   "source": [
    "seed1 = \"Patrick Mahomes\"\n",
    "output1 = generate_text(model, seed1, num_chars=100)\n",
    "seed2 = \"americano\"\n",
    "output2 = generate_text(model, seed2, num_chars=100)\n",
    "print('Prueba 2:',output1)\n",
    "print('Prueba 1:',output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nNi4Zn1858N"
   },
   "source": [
    "En Keras se puede guardar todo el modelo (estructura + pesos + optimizador)\n",
    "\n",
    "Esto permite recargar el modelo completo después, tal como estaba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TbWl-FC85cDP"
   },
   "outputs": [],
   "source": [
    "# Guardar el modelo completo\n",
    "model.save_weights(\"NFL_GPT.keras\")\n",
    "# Cargar el modelo completo\n",
    "modelo_cargado = load_model(\"NFL_GPT.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvkhitMh0tFM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOh7dI+YesnxeEXMaSV44Q2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
